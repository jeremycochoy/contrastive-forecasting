{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837bb1d4",
   "metadata": {
    "id": "837bb1d4"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from types import SimpleNamespace\n",
    "\n",
    "# Import functions from codebase modules\n",
    "from arma import generate_arma_batch\n",
    "from loss import contrastive_latent_loss\n",
    "from network import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9381a71",
   "metadata": {
    "id": "d9381a71"
   },
   "outputs": [],
   "source": [
    "# Data generation wrapper for backward compatibility\n",
    "def generate_random_walk(batch_size=16, T_raw=4096, C=4, mean=0.0, std=1.0, seed=None):\n",
    "    \"\"\"Generate multivariate random walks using ARMA processes.\"\"\"\n",
    "    X, _ = generate_arma_batch(batch_size=batch_size, T_raw=T_raw, C=C, mean=mean, std=std, seed=seed)\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a6d9dd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "22a6d9dd",
    "outputId": "0ed51b18-6637-4fd3-e1f1-b181ed6a6f1a"
   },
   "outputs": [],
   "source": [
    "# Generate sample data and initialize model\n",
    "x = generate_random_walk(batch_size=32, T_raw=4096)\n",
    "print(f\"Data shape: {x.shape}\")\n",
    "\n",
    "model = SimpleModel(C=4, H=64, W=32)\n",
    "print(f\"Model initialized: {model.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afpesKUH8-sD",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "id": "afpesKUH8-sD",
    "outputId": "0b23a8f3-f5c2-4504-b37a-95dcf9e4cc08"
   },
   "outputs": [],
   "source": [
    "# Visualize sample data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(np.cumsum(x[0, :, :], axis=0))\n",
    "plt.title('Sample ARMA Process (Cumulative Sum)')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Cumulative Value')\n",
    "plt.legend([f'Channel {i}' for i in range(x.shape[2])])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763c4c11",
   "metadata": {
    "id": "763c4c11"
   },
   "outputs": [],
   "source": [
    "# Test model forward pass\n",
    "h_hat, h = model(x)\n",
    "print(f\"Forecasted latent shape: {h_hat.shape}\")\n",
    "print(f\"Original latent shape: {h.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05e1966",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c05e1966",
    "outputId": "d64d8719-bd13-45d5-88ca-9e80ff4c8b20"
   },
   "outputs": [],
   "source": [
    "# Test loss computation\n",
    "spec = SimpleNamespace(\n",
    "    train_configuration={\n",
    "        'contrastive_divergence_temperature': 0.05,\n",
    "        'contrastive_latent_noise': None,\n",
    "        'loss_shape': 'cosine_similarity'\n",
    "    }\n",
    ")\n",
    "\n",
    "loss = contrastive_latent_loss(\n",
    "    predicted_position=[h, h_hat],\n",
    "    validation=False,\n",
    "    spec=spec,\n",
    "    get_history=False\n",
    ")\n",
    "\n",
    "print(f\"Contrastive latent loss: {loss.item():.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00291a6-23ff-4c58-aba7-a5ab50841989",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_state():\n",
    "    \"\"\"Create a new training state dictionary.\"\"\"\n",
    "    return {\n",
    "        'steps': [],\n",
    "        'train_metrics': {'ff': [], 'fp': [], 'tp': []},\n",
    "        'val_metrics': {'ff': [], 'fp': [], 'tp': []},\n",
    "        'current_step': 0,\n",
    "        'model': None,\n",
    "        'optimizer': None,\n",
    "        'x_val': None,\n",
    "        'spec': None,\n",
    "        'cld': None\n",
    "    }\n",
    "\n",
    "def setup_training(model, C, H, W, batch_size, device, lr=1e-4):\n",
    "    \"\"\"Initialize training setup with model, optimizer, and validation data.\"\"\"\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "    \n",
    "    # Fixed validation set\n",
    "    x_val = generate_random_walk(batch_size, T_raw=4096, C=C, seed=0).to(device)\n",
    "    Bv, Tr, _ = x_val.shape\n",
    "    T = Tr // W\n",
    "    xt_val = x_val.view(Bv, T, W, C).permute(0,1,3,2)  # [Bv, T, C, W]\n",
    "    \n",
    "    # Training configuration\n",
    "    spec = SimpleNamespace(train_configuration={\n",
    "        'contrastive_divergence_temperature': 0.07,\n",
    "        'contrastive_latent_noise': None,\n",
    "        'loss_shape': 'cosine_similarity',\n",
    "        'contrastive_latent_delay': 0\n",
    "    })\n",
    "    cld = spec.train_configuration['contrastive_latent_delay'] + 1\n",
    "    \n",
    "    return model, optimizer, xt_val, spec, cld\n",
    "\n",
    "def compute_metrics(f_lat, o_lat, cld):\n",
    "    \"\"\"Compute forecast-future, forecast-past, and future-past cosine similarities.\"\"\"\n",
    "    fn = F.normalize(f_lat, p=2, dim=-1)\n",
    "    on = F.normalize(o_lat, p=2, dim=-1)\n",
    "    hyh = fn[:, :-cld, :, :]\n",
    "    hyn = on[:,  cld:, :, :]\n",
    "    hxn = on[:, :-cld, :, :]\n",
    "    \n",
    "    ff = (hyh * hyn).sum(-1).mean().item()\n",
    "    fp = (hyh * hxn).sum(-1).mean().item()\n",
    "    tp = (hyn * hxn).sum(-1).mean().item()\n",
    "    \n",
    "    return ff, fp, tp\n",
    "\n",
    "def train_step(model, optimizer, loss_fn, C, H, W, batch_size, device, spec):\n",
    "    \"\"\"Execute a single training step.\"\"\"\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Generate training batch\n",
    "    x_train = generate_random_walk(batch_size, T_raw=4096, C=C).to(device)\n",
    "    Bt, _, _ = x_train.shape\n",
    "    T = x_train.shape[1] // W\n",
    "    xt = x_train.view(Bt, T, W, C).permute(0,1,3,2)  # [Bt, T, C, W]\n",
    "    \n",
    "    # Forward pass\n",
    "    f_flat, o_flat = model.transformer(xt)\n",
    "    # f_flat and o_flat have shape [Bt*C, T, H]\n",
    "    f_lat = f_flat.reshape(Bt, C, T, H).permute(0,2,1,3)  # [Bt, T, C, H]\n",
    "    o_lat = o_flat.reshape(Bt, C, T, H).permute(0,2,1,3)  # [Bt, T, C, H]\n",
    "    \n",
    "    # Compute loss and backprop\n",
    "    loss = loss_fn((f_lat, o_lat), validation=False, spec=spec)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item(), f_lat, o_lat\n",
    "\n",
    "def validation_step(model, x_val, loss_fn, spec, cld):\n",
    "    \"\"\"Execute validation step.\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        fv_flat, ov_flat = model.transformer(x_val)\n",
    "        Bv, T, C, W = x_val.shape  # x_val has shape [Bv, T, C, W]\n",
    "        H = model.H  # Get H from model\n",
    "        \n",
    "        # Reshape the flat outputs correctly\n",
    "        # fv_flat and ov_flat have shape [Bv*C, T, H]\n",
    "        fv = fv_flat.reshape(Bv, C, T, H).permute(0,2,1,3)  # [Bv, T, C, H]\n",
    "        ov = ov_flat.reshape(Bv, C, T, H).permute(0,2,1,3)  # [Bv, T, C, H]\n",
    "        \n",
    "        return compute_metrics(fv, ov, cld)\n",
    "\n",
    "def plot_training_curves(training_state):\n",
    "    \"\"\"Plot training metrics curves.\"\"\"\n",
    "    plt.figure(figsize=(8,4))\n",
    "    steps = range(1, len(training_state['train_metrics']['ff'])+1)\n",
    "    plt.plot(steps, training_state['train_metrics']['ff'], label='Train Forecast vs Future')\n",
    "    plt.plot(steps, training_state['train_metrics']['fp'], label='Train Forecast vs Past')\n",
    "    plt.plot(steps, training_state['train_metrics']['tp'], label='Train Future vs Past')\n",
    "    plt.xlabel('Step')\n",
    "    plt.ylabel('Mean Cosine Similarity')\n",
    "    plt.title('Training Metrics')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_validation_curves(training_state):\n",
    "    \"\"\"Plot validation metrics curves.\"\"\"\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.plot(training_state['steps'], training_state['val_metrics']['ff'], label='Val Forecast vs Future')\n",
    "    plt.plot(training_state['steps'], training_state['val_metrics']['fp'], label='Val Forecast vs Past')\n",
    "    plt.plot(training_state['steps'], training_state['val_metrics']['tp'], label='Val Future vs Past')\n",
    "    plt.xlabel('Step')\n",
    "    plt.ylabel('Mean Cosine Similarity')\n",
    "    plt.title('Validation Metrics')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def train_model(model, loss_fn, C, H, W, total_steps=500, batch_size=8, \n",
    "                lr=1e-4, device='cpu', val_every=50, training_state=None):\n",
    "    \"\"\"\n",
    "    Train model with re-entry capability.\n",
    "    \n",
    "    Args:\n",
    "        training_state: Dictionary containing training state. If None, creates a new one.\n",
    "    \"\"\"\n",
    "    # Create or use provided training state\n",
    "    if training_state is None:\n",
    "        training_state = create_training_state()\n",
    "    \n",
    "    # Setup training if not already done\n",
    "    if training_state['model'] is None:\n",
    "        model, optimizer, x_val, spec, cld = setup_training(model, C, H, W, batch_size, device, lr)\n",
    "        training_state.update({\n",
    "            'model': model,\n",
    "            'optimizer': optimizer,\n",
    "            'x_val': x_val,\n",
    "            'spec': spec,\n",
    "            'cld': cld,\n",
    "            'current_step': 0\n",
    "        })\n",
    "    else:\n",
    "        model = training_state['model']\n",
    "        optimizer = training_state['optimizer']\n",
    "        x_val = training_state['x_val']\n",
    "        spec = training_state['spec']\n",
    "        cld = training_state['cld']\n",
    "    \n",
    "    current_step = training_state['current_step']\n",
    "    \n",
    "    # Training loop\n",
    "    for step in range(current_step + 1, total_steps + 1):\n",
    "        # Training step\n",
    "        loss_val, f_lat, o_lat = train_step(model, optimizer, loss_fn, C, H, W, batch_size, device, spec)\n",
    "        \n",
    "        # Compute training metrics\n",
    "        train_ff, train_fp, train_tp = compute_metrics(f_lat, o_lat, cld)\n",
    "        training_state['train_metrics']['ff'].append(train_ff)\n",
    "        training_state['train_metrics']['fp'].append(train_fp)\n",
    "        training_state['train_metrics']['tp'].append(train_tp)\n",
    "        \n",
    "        # Validation step\n",
    "        if step % val_every == 0 or step == total_steps:\n",
    "            val_ff, val_fp, val_tp = validation_step(model, x_val, loss_fn, spec, cld)\n",
    "            training_state['val_metrics']['ff'].append(val_ff)\n",
    "            training_state['val_metrics']['fp'].append(val_fp)\n",
    "            training_state['val_metrics']['tp'].append(val_tp)\n",
    "            training_state['steps'].append(step)\n",
    "            \n",
    "            print(f\"[Step {step}] train loss {loss_val:.4f} | \"\n",
    "                  f\"train FF={train_ff:.4f}, FP={train_fp:.4f}, TP={train_tp:.4f} || \"\n",
    "                  f\"val   FF={val_ff:.4f}, FP={val_fp:.4f}, TP={val_tp:.4f}\")\n",
    "            \n",
    "            # Plot curves\n",
    "            try:\n",
    "                plot_training_curves(training_state)\n",
    "                plot_validation_curves(training_state)\n",
    "            except Exception as e:\n",
    "                print(f\"Plotting error: {e}\")\n",
    "        else:\n",
    "            print(f\"[Step {step}] train loss {loss_val:.4f}\")\n",
    "        \n",
    "        # Update current step\n",
    "        training_state['current_step'] = step\n",
    "    \n",
    "    # Final plots\n",
    "    plot_training_curves(training_state)\n",
    "    plot_validation_curves(training_state)\n",
    "    \n",
    "    return model, training_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0jREo4VTFLUR",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "0jREo4VTFLUR",
    "outputId": "31679359-8a03-49fc-dad8-0d599da20aa0"
   },
   "outputs": [],
   "source": [
    "# Initialize model and start training\n",
    "model = SimpleModel(C=4, H=64, W=32)\n",
    "trained_model, training_state = train_model(\n",
    "    model,\n",
    "    loss_fn=contrastive_latent_loss,\n",
    "    C=4, H=64, W=32,\n",
    "    total_steps=500_000,\n",
    "    batch_size=32,\n",
    "    lr=1e-4,\n",
    "    device='cuda',\n",
    "    val_every=500,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8545642c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Resume training from where you left off (re-entry capability)\n",
    "# This allows you to interrupt training and continue from where you left off\n",
    "\n",
    "print(f\"Current step: {training_state['current_step']}\")\n",
    "print(f\"Training metrics length: {len(training_state['train_metrics']['ff'])}\")\n",
    "print(f\"Validation steps: {training_state['steps']}\")\n",
    "\n",
    "# To resume training from where you left off:\n",
    "# resumed_model, updated_state = train_model(\n",
    "#     model,\n",
    "#     loss_fn=contrastive_latent_loss,\n",
    "#     C=4, H=64, W=32,\n",
    "#     total_steps=1000,  # Continue to step 1000\n",
    "#     batch_size=32,\n",
    "#     lr=1e-4,\n",
    "#     device='cuda',\n",
    "#     val_every=500,\n",
    "#     training_state=training_state  # Pass the existing state\n",
    "# )\n",
    "\n",
    "# To start fresh training with a new state:\n",
    "# new_model = SimpleModel(C=4, H=64, W=32)\n",
    "# fresh_model, new_state = train_model(\n",
    "#     new_model,\n",
    "#     loss_fn=contrastive_latent_loss,\n",
    "#     C=4, H=64, W=32,\n",
    "#     total_steps=1000,\n",
    "#     batch_size=32,\n",
    "#     lr=1e-4,\n",
    "#     device='cuda',\n",
    "#     val_every=500,\n",
    "#     training_state=None  # Creates a new state\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be625e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V5E1",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
