{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARMA Parameter Recovery Experiment\n",
    "\n",
    "This notebook trains a linear head to recover ARMA model parameters from the trained SimpleModel's latent representations.\n",
    "\n",
    "## Overview\n",
    "- Load the pre-trained SimpleModel from forecast_arma.ipynb\n",
    "- Create a linear head that outputs 2 tensors of 8 floats (AR and MA parameters)\n",
    "- Train the head to recover the original ARMA parameters\n",
    "- Evaluate performance during and after training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" # 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from types import SimpleNamespace\n",
    "\n",
    "# Import functions from codebase modules\n",
    "from arma import generate_arma_batch\n",
    "from network import SimpleModel\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION: ARMA Parameter Recovery Settings\n",
    "# =============================================================================\n",
    "NUM_ARMA_PARAMS = 8  # Number of AR and MA coefficients to recover (change this to control the experiment)\n",
    "# =============================================================================\n",
    "print(f\"Configuration: NUM_ARMA_PARAMS = {NUM_ARMA_PARAMS}\")\n",
    "print(\"Change NUM_ARMA_PARAMS in this cell to control the number of AR/MA coefficients to recover\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model\n",
    "model = SimpleModel(C=4, H=1024, W=32)\n",
    "model.load_state_dict(torch.load('trained_simple_model_H1024.pth', map_location=device))\n",
    "model = model.to(device)\n",
    "model.eval()  # Set to evaluation mode\n",
    "print(\"Pre-trained model loaded successfully\")\n",
    "\n",
    "# Freeze the pre-trained model parameters\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "print(\"Pre-trained model parameters frozen\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParameterRecoveryHead(nn.Module):\n",
    "    \"\"\"Linear head to recover ARMA parameters from latent representations.\n",
    "    Works on flattened C*H representation: [B, T, C*H] -> [B, T, num_arma_params]\"\"\"\n",
    "    \n",
    "    def __init__(self, C=4, H=64, hidden_dim=64, num_arma_params=8):\n",
    "        super().__init__()\n",
    "        input_dim = H  # H flattened dimension\n",
    "        \n",
    "        # Linear head that works on C*H dimension\n",
    "        # Input: [B, T, C*H] -> [B, T, hidden_dim]\n",
    "        self.shared_layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.CELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.CELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.CELU(),\n",
    "            nn.Dropout(0.1),\n",
    "        )\n",
    "        \n",
    "        # Separate heads for AR and MA parameters\n",
    "        # Output per (B, T): num_arma_params AR + num_arma_params MA = 2*num_arma_params values\n",
    "        self.ar_head = nn.Linear(hidden_dim, num_arma_params)  # num_arma_params AR parameters\n",
    "        self.ma_head = nn.Linear(hidden_dim, num_arma_params)  # num_arma_params MA parameters\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor of shape [batch_size, T, C*H] (flattened C and H)\n",
    "        Returns:\n",
    "            ar_params: AR parameters [batch_size, T, num_arma_params]\n",
    "            ma_params: MA parameters [batch_size, T, num_arma_params]\n",
    "        \"\"\"\n",
    "        # Apply linear head to C*H dimension, vectorized over B, T\n",
    "        # shared_features: [B, T, hidden_dim]\n",
    "        shared_features = self.shared_layers(x)\n",
    "        \n",
    "        # Predict parameters for each (B, T) position\n",
    "        # ar_predictions: [B, T, num_arma_params], ma_predictions: [B, T, num_arma_params]\n",
    "        ar_predictions = torch.tanh(self.ar_head(shared_features))\n",
    "        ma_predictions = torch.tanh(self.ma_head(shared_features))\n",
    "        \n",
    "        return ar_predictions, ma_predictions\n",
    "\n",
    "# Initialize the parameter recovery head\n",
    "# Head works on C*H dimension (4*256=1024)\n",
    "param_head = ParameterRecoveryHead(C=4, H=256, hidden_dim=256, num_arma_params=NUM_ARMA_PARAMS).to(device)\n",
    "print(f\"Parameter recovery head initialized with C=4, H=256 (input_dim=1024), num_params={NUM_ARMA_PARAMS}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_latent_features(model, x):\n",
    "    \"\"\"Extract latent features from the pre-trained model and reshape to C*H.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        # Get latent representations from the model\n",
    "        h_hat, h = model(x)  # h_hat: forecasted, h: original\n",
    "        \n",
    "        # Use the original latent representation (h)\n",
    "        # Reshape from [batch_size, T, C, H] to [batch_size, T, C*H]\n",
    "        # This flattens C and H together since they are sampled the same and correlated\n",
    "        B, T, C, H = h.shape\n",
    "        h_reshaped = h.reshape(B, T, C * H)\n",
    "        return h_reshaped\n",
    "\n",
    "def parameter_loss(pred_ar, pred_ma, true_ar, true_ma):\n",
    "    \"\"\"Compute loss between predicted and true parameters.\n",
    "    \n",
    "    Args:\n",
    "        pred_ar: [B, T, NUM_ARMA_PARAMS] - predicted AR parameters\n",
    "        pred_ma: [B, T, NUM_ARMA_PARAMS] - predicted MA parameters\n",
    "        true_ar: [B, NUM_ARMA_PARAMS] - true AR parameters\n",
    "        true_ma: [B, NUM_ARMA_PARAMS] - true MA parameters\n",
    "    \n",
    "    Returns:\n",
    "        total_loss: combined AR + MA loss\n",
    "        ar_loss: AR loss\n",
    "        ma_loss: MA loss\n",
    "    \"\"\"\n",
    "    B, T, _ = pred_ar.shape\n",
    "    \n",
    "    # Average over time dimension to get [B, NUM_ARMA_PARAMS]\n",
    "    # This aggregates predictions across all patches on the time dimension\n",
    "    pred_ar_avg = pred_ar.mean(dim=1)  # [B, NUM_ARMA_PARAMS]\n",
    "    pred_ma_avg = pred_ma.mean(dim=1)  # [B, NUM_ARMA_PARAMS]\n",
    "    \n",
    "    # Compute loss between averaged predictions [B, NUM_ARMA_PARAMS] and true [B, NUM_ARMA_PARAMS]\n",
    "    ar_loss = F.mse_loss(pred_ar_avg, true_ar)\n",
    "    ma_loss = F.mse_loss(pred_ma_avg, true_ma)\n",
    "    \n",
    "    return ar_loss + ma_loss, ar_loss, ma_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_training_data(batch_size=32, T_raw=4096, C=4, seed=None):\n",
    "    \"\"\"Generate training data with known ARMA parameters.\"\"\"\n",
    "    # Generate ARMA data with known parameters\n",
    "    x, parameters = generate_arma_batch(batch_size=batch_size, T_raw=T_raw, C=C, seed=seed)\n",
    "    x = x.to(device)\n",
    "    \n",
    "    # Extract true parameters\n",
    "    true_ar_params = []\n",
    "    true_ma_params = []\n",
    "    \n",
    "    for ar_poly, ma_poly in parameters:\n",
    "        # Convert polynomial form to parameter form\n",
    "        # AR: 1 - φ1*L - φ2*L^2 - ... -> [φ1, φ2, ...]\n",
    "        # MA: 1 + θ1*L + θ2*L^2 + ... -> [θ1, θ2, ...]\n",
    "        ar_coeffs = -ar_poly[1:]  # Remove constant term and negate\n",
    "        ma_coeffs = ma_poly[1:]   # Remove constant term\n",
    "        \n",
    "        # Pad or truncate to exactly NUM_ARMA_PARAMS parameters\n",
    "        ar_padded = np.pad(ar_coeffs, (0, max(0, NUM_ARMA_PARAMS - len(ar_coeffs))), mode='constant')[:NUM_ARMA_PARAMS]\n",
    "        ma_padded = np.pad(ma_coeffs, (0, max(0, NUM_ARMA_PARAMS - len(ma_coeffs))), mode='constant')[:NUM_ARMA_PARAMS]\n",
    "        \n",
    "        true_ar_params.append(ar_padded)\n",
    "        true_ma_params.append(ma_padded)\n",
    "    \n",
    "    true_ar = torch.tensor(np.array(true_ar_params), dtype=torch.float32).to(device)\n",
    "    true_ma = torch.tensor(np.array(true_ma_params), dtype=torch.float32).to(device)\n",
    "    \n",
    "    return x, true_ar, true_ma\n",
    "\n",
    "# Test data generation\n",
    "x_test, ar_test, ma_test = prepare_training_data(batch_size=4, seed=42)\n",
    "print(f\"Test data shapes: x={x_test.shape}, ar={ar_test.shape}, ma={ma_test.shape}\")\n",
    "print(f\"Sample AR params: {ar_test[0]}\")\n",
    "print(f\"Sample MA params: {ma_test[0]}\")\n",
    "print(f\"AR L1 norm: {torch.norm(ar_test[0], p=1):.4f}\")\n",
    "print(f\"MA L1 norm: {torch.norm(ma_test[0], p=1):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_parameter_recovery(param_head, model, num_epochs=100, batch_size=32, lr=1e-3):\n",
    "    \"\"\"Train the parameter recovery head.\"\"\"\n",
    "    optimizer = optim.Adam(param_head.parameters(), lr=lr)\n",
    "    \n",
    "    # Training history\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    ar_losses = []\n",
    "    ma_losses = []\n",
    "    \n",
    "    # Generate validation set\n",
    "    x_val, ar_val, ma_val = prepare_training_data(batch_size=batch_size, seed=0)\n",
    "    h_val = extract_latent_features(model, x_val)\n",
    "    \n",
    "    print(\"Starting parameter recovery training...\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training step\n",
    "        param_head.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Generate training batch\n",
    "        x_train, ar_train, ma_train = prepare_training_data(batch_size=batch_size, seed=epoch)\n",
    "        h_train = extract_latent_features(model, x_train)\n",
    "        \n",
    "        # Forward pass\n",
    "        pred_ar, pred_ma = param_head(h_train)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss, ar_loss, ma_loss = parameter_loss(pred_ar, pred_ma, ar_train, ma_train)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Validation step\n",
    "        param_head.eval()\n",
    "        with torch.no_grad():\n",
    "            pred_ar_val, pred_ma_val = param_head(h_val)\n",
    "            val_loss, _, _ = parameter_loss(pred_ar_val, pred_ma_val, ar_val, ma_val)\n",
    "        \n",
    "        # Record metrics\n",
    "        train_losses.append(loss.item())\n",
    "        val_losses.append(val_loss.item())\n",
    "        ar_losses.append(ar_loss.item())\n",
    "        ma_losses.append(ma_loss.item())\n",
    "        \n",
    "        # Print progress\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}: \"\n",
    "                  f\"Train Loss: {loss.item():.6f}, \"\n",
    "                  f\"Val Loss: {val_loss.item():.6f}, \"\n",
    "                  f\"AR Loss: {ar_loss.item():.6f}, \"\n",
    "                  f\"MA Loss: {ma_loss.item():.6f}\")\n",
    "    \n",
    "    return train_losses, val_losses, ar_losses, ma_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "train_losses, val_losses, ar_losses, ma_losses = train_parameter_recovery(\n",
    "    param_head, model, num_epochs=30_000, batch_size=32, lr=1e-3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(train_losses, label='Train Loss', alpha=0.8)\n",
    "plt.plot(val_losses, label='Val Loss', alpha=0.8)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Total Loss')\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(ar_losses, label='AR Loss', alpha=0.8)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('AR Parameter Loss')\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(ma_losses, label='MA Loss', alpha=0.8)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('MA Parameter Loss')\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate final performance\n",
    "def evaluate_parameter_recovery(param_head, model, num_samples=100):\n",
    "    \"\"\"Evaluate parameter recovery performance on test samples.\"\"\"\n",
    "    param_head.eval()\n",
    "    \n",
    "    all_ar_errors = []\n",
    "    all_ma_errors = []\n",
    "    all_total_errors = []\n",
    "    all_baseline_errors = []  # Error if predicting 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(num_samples):\n",
    "            try:\n",
    "                # Generate test sample\n",
    "                x_test, ar_true, ma_true = prepare_training_data(batch_size=1, seed=i+1000)\n",
    "                h_test = extract_latent_features(model, x_test)\n",
    "                \n",
    "                # Predict parameters: [B, T, NUM_ARMA_PARAMS]\n",
    "                pred_ar, pred_ma = param_head(h_test)\n",
    "                \n",
    "                # Compute errors using parameter_loss function\n",
    "                _, ar_error, ma_error = parameter_loss(pred_ar, pred_ma, ar_true, ma_true)\n",
    "                ar_error = ar_error.item()\n",
    "                ma_error = ma_error.item()\n",
    "                total_error = ar_error + ma_error\n",
    "                \n",
    "                # Compute baseline error (predicting 0)\n",
    "                baseline_ar_error = F.mse_loss(torch.zeros_like(ar_true), ar_true).item()\n",
    "                baseline_ma_error = F.mse_loss(torch.zeros_like(ma_true), ma_true).item()\n",
    "                baseline_total_error = baseline_ar_error + baseline_ma_error\n",
    "                \n",
    "                all_ar_errors.append(ar_error)\n",
    "                all_ma_errors.append(ma_error)\n",
    "                all_total_errors.append(total_error)\n",
    "                all_baseline_errors.append(baseline_total_error)\n",
    "            except Exception as e:\n",
    "                print(f\"Error in sample {i}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    if not all_ar_errors:\n",
    "        raise RuntimeError(\"No valid samples processed during evaluation\")\n",
    "    \n",
    "    return {\n",
    "        'ar_errors': all_ar_errors,\n",
    "        'ma_errors': all_ma_errors,\n",
    "        'total_errors': all_total_errors,\n",
    "        'baseline_errors': all_baseline_errors,\n",
    "        'mean_ar_error': np.mean(all_ar_errors),\n",
    "        'mean_ma_error': np.mean(all_ma_errors),\n",
    "        'mean_total_error': np.mean(all_total_errors),\n",
    "        'mean_baseline_error': np.mean(all_baseline_errors),\n",
    "        'std_ar_error': np.std(all_ar_errors),\n",
    "        'std_ma_error': np.std(all_ma_errors),\n",
    "        'std_total_error': np.std(all_total_errors),\n",
    "        'std_baseline_error': np.std(all_baseline_errors)\n",
    "    }\n",
    "\n",
    "# Run evaluation\n",
    "results = evaluate_parameter_recovery(param_head, model, num_samples=200)\n",
    "\n",
    "print(\"\\n=== Parameter Recovery Performance ===\")\n",
    "print(f\"Mean AR Error: {results['mean_ar_error']:.6f} ± {results['std_ar_error']:.6f}\")\n",
    "print(f\"Mean MA Error: {results['mean_ma_error']:.6f} ± {results['std_ma_error']:.6f}\")\n",
    "print(f\"Mean Total Error: {results['mean_total_error']:.6f} ± {results['std_total_error']:.6f}\")\n",
    "print(f\"\\n=== Baseline (predicting 0) ===\")\n",
    "print(f\"Mean Baseline Error: {results['mean_baseline_error']:.6f} ± {results['std_baseline_error']:.6f}\")\n",
    "print(f\"Improvement Ratio: {results['mean_baseline_error'] / results['mean_total_error']:.2f}x better than baseline\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize parameter recovery quality\n",
    "def plot_parameter_comparison(param_head, model, num_examples=5):\n",
    "    \"\"\"Plot comparison between true and predicted parameters.\"\"\"\n",
    "    param_head.eval()\n",
    "    \n",
    "    fig, axes = plt.subplots(num_examples, 2, figsize=(12, 3*num_examples))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(num_examples):\n",
    "            # Generate test sample\n",
    "            x_test, ar_true, ma_true = prepare_training_data(batch_size=1, seed=i+2000)\n",
    "            h_test = extract_latent_features(model, x_test)\n",
    "            \n",
    "            # Predict parameters: [1, T, NUM_ARMA_PARAMS]\n",
    "            pred_ar, pred_ma = param_head(h_test)\n",
    "            \n",
    "            # Average over T to get single [NUM_ARMA_PARAMS] prediction for plotting\n",
    "            pred_ar_mean = pred_ar[0].mean(dim=0).cpu().numpy()  # [NUM_ARMA_PARAMS]\n",
    "            pred_ma_mean = pred_ma[0].mean(dim=0).cpu().numpy()  # [NUM_ARMA_PARAMS]\n",
    "            \n",
    "            # Convert to numpy for plotting\n",
    "            ar_true_np = ar_true[0].cpu().numpy()\n",
    "            ma_true_np = ma_true[0].cpu().numpy()\n",
    "            ar_pred_np = pred_ar_mean\n",
    "            ma_pred_np = pred_ma_mean\n",
    "            \n",
    "            # Plot AR parameters\n",
    "            axes[i, 0].bar(range(NUM_ARMA_PARAMS), ar_true_np, alpha=0.7, label='True', color='blue')\n",
    "            axes[i, 0].bar(range(NUM_ARMA_PARAMS), ar_pred_np, alpha=0.7, label='Predicted', color='red')\n",
    "            axes[i, 0].set_title(f'AR Parameters - Sample {i+1}')\n",
    "            axes[i, 0].set_xlabel('Parameter Index')\n",
    "            axes[i, 0].set_ylabel('Value')\n",
    "            axes[i, 0].legend()\n",
    "            axes[i, 0].grid(True, alpha=0.3)\n",
    "            \n",
    "            # Plot MA parameters\n",
    "            axes[i, 1].bar(range(NUM_ARMA_PARAMS), ma_true_np, alpha=0.7, label='True', color='blue')\n",
    "            axes[i, 1].bar(range(NUM_ARMA_PARAMS), ma_pred_np, alpha=0.7, label='Predicted', color='red')\n",
    "            axes[i, 1].set_title(f'MA Parameters - Sample {i+1}')\n",
    "            axes[i, 1].set_xlabel('Parameter Index')\n",
    "            axes[i, 1].set_ylabel('Value')\n",
    "            axes[i, 1].legend()\n",
    "            axes[i, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot parameter comparisons\n",
    "plot_parameter_comparison(param_head, model, num_examples=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error distribution analysis\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.hist(results['ar_errors'], bins=30, alpha=0.7, color='blue')\n",
    "plt.xlabel('AR Parameter MSE')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('AR Parameter Error Distribution')\n",
    "plt.axvline(results['mean_ar_error'], color='red', linestyle='--', label=f'Mean: {results[\"mean_ar_error\"]:.4f}')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.hist(results['ma_errors'], bins=30, alpha=0.7, color='green')\n",
    "plt.xlabel('MA Parameter MSE')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('MA Parameter Error Distribution')\n",
    "plt.axvline(results['mean_ma_error'], color='red', linestyle='--', label=f'Mean: {results[\"mean_ma_error\"]:.4f}')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.hist(results['total_errors'], bins=30, alpha=0.7, color='purple')\n",
    "plt.xlabel('Total Parameter MSE')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Total Parameter Error Distribution')\n",
    "plt.axvline(results['mean_total_error'], color='red', linestyle='--', label=f'Mean: {results[\"mean_total_error\"]:.4f}')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained parameter recovery head\n",
    "torch.save(param_head.state_dict(), 'parameter_recovery_head.pth')\n",
    "print(\"Parameter recovery head saved to 'parameter_recovery_head.pth'\")\n",
    "\n",
    "# Summary of results\n",
    "print(\"\\n=== Experiment Summary ===\")\n",
    "if train_losses and val_losses:\n",
    "    print(f\"Final training loss: {train_losses[-1]:.6f}\")\n",
    "    print(f\"Final validation loss: {val_losses[-1]:.6f}\")\n",
    "else:\n",
    "    print(\"Training not completed - no loss values available\")\n",
    "\n",
    "if 'results' in locals() and results:\n",
    "    print(f\"Mean AR parameter recovery error: {results['mean_ar_error']:.6f}\")\n",
    "    print(f\"Mean MA parameter recovery error: {results['mean_ma_error']:.6f}\")\n",
    "    print(f\"Mean total parameter recovery error: {results['mean_total_error']:.6f}\")\n",
    "else:\n",
    "    print(\"Evaluation not completed - no results available\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
